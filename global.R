################################################################################################################################################################################################################################################################

# Load packages
if(!require(shiny)) {install.packages("shiny"); require(shiny)}
if(!require(tidyverse)) {install.packages("tidyverse"); require(tidyverse)}
if(!require(xgboost)) {install.packages("xgboost"); require(xgboost)}
if(!require(ModelMetrics)) {install.packages("ModelMetrics"); require(ModelMetrics)}
if(!require(shinythemes)) {install.packages("shinythemes"); require(shinythemes)}

################################################################################################################################################################################################################################################################

# Prepare data
x_train <- iris %>% select(-Species)
y_train <- as.numeric(factor(iris$Species)) - 1
x_test <- iris %>% select(-Species)
y_test <- as.numeric(factor(iris$Species)) - 1

################################################################################################################################################################################################################################################################

# Save variable names
var.names = names(x_train)
x_train <- as.matrix(x_train)
x_test <- as.matrix(x_test)

################################################################################################################################################################################################################################################################

# Prepare xgboost parameters
param <- list("objective" = "multi:softprob", 
              "eval_metric" = "mlogloss", 
              "num_class" = length(table(y_train)), 
              "eta" = .2,
              "max_depth" = 3, 
              "lambda" = 1, 
              "alpha" = 2,
              "min_child_weight" = 11, 
              "subsample" = .9, 
              "colsample_bytree" = .7)

################################################################################################################################################################################################################################################################

# Set up cross validation
bst.cv <- xgb.cv(param = param, data = x_train, label = y_train, nfold = 5, nrounds = 20000, missing = NA, prediction = TRUE)

################################################################################################################################################################################################################################################################

# Select which round(s) had the lowest/best log loss value
nround <- which(bst.cv$evaluation_log$test_mlogloss_mean == min(bst.cv$evaluation_log$test_mlogloss_mean))

################################################################################################################################################################################################################################################################

# Build classifier
irisClassifier <- xgboost(params = param, data = x_train, label = y_train, nrounds = nround, missing = NA)

################################################################################################################################################################################################################################################################
